## Validating the Human-Interpretability of Circuit Feature Visualization

### Project Members
- James Lucassen
- Josh Cordova
- Sofiane Dissem
- Theo Bayard de Volo

### Video
<p align="center">
<iframe width="695" height="391" src="https://www.youtube.com/embed/XA4zV3WZxjE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</p>

### Abstract
Neural networks are traditionally seen as black boxes where information goes in and a response comes out, with no knowledge of the internal mechansim. However, the recent [Circuits](https://distill.pub/2020/circuits/) research has suggested that we can visualize the behaviors of individual neurons in convolutional neural networks. These neurons correspond to human-readable concepts, which then connect to each other in "circuits" that construct more sophisticated concepts, eventually building up to features that construct something high-level (such as recognizing dogs). In this paper, we aim to determine whether the concepts corresponding to distinct neurons are really more interpretable to humans than concepts generated by random linear combinations of neurons. We do this by generating visualizations of each type, and performing a survey using Amazon Mechanical Turk asking participants to choose the more interpretable image out of a series of pairs. We failed to find statistically significant results, which challenges this key result in the transparency literature. However, further study is still needed - in particular, we would like to follow up with greater statistical power and higher data quality.

### Ethics
Treating neural networks (or any machine learning system) as a black box introduces a number of problems. If we do not understand the "reasoning" inside a neural network, it makes it much harder to verify whether it is robust, fair, or aligned with our intentions. Significant results in neural network transparency could have major ethical implications, for both modern data ethics and the long-term safety of AI. A more thorough body of research validating the promising but new Circuits method could bring a powerful new transparency tool to a more mature stage of development, ready for application. Regarding modern data ethics, transparency procedures are valuable for ensuring machine learning models act fairly before deployment, providing evidence in appeals against unfair machine decisions, and a variety of other applications. And in the long term, as AI systems are given more important roles and grow more complicated, transparency is likely to be crucial for designing validation processes which will ensure that they never fail catastrophically.

### Review of Literature
The key technique that’s central to the circuits method of transparency is called feature visualization by image optimization. At a high level, this technique can break down a neural network into sub-components by using visualizations to identify what each part of the network is “looking for”. It works by choosing an objective function expressed in terms of the activations of various neurons within the network to be analyzed, and using a convolutional neural network to gradually optimize an image to produce an activation that matches the objective. These objectives can be represented as vectors in a high-dimensional activation space, where each spatial coordinate is the activation of one of the neurons in the target network. An objective function that focuses on maximizing the activation of a particular neuron, then, would be a unit vector in one of the basis directions of the activation space. The activations produced by the visualization process would almost certainly not be a basis vector – just some vector that has a particularly high activation on this one dimension. Using this formalism, the visualization by optimization technique works by maximizing the dot product of the objective activation vector and the actual activation vector produced by the image.

The basic idea behing this technique originally came out of [Google’s Deep Dream]( https://ai.googleblog.com/2015/07/deepdream-code-example-for-visualizing.html) project, which optimized images to maximize the outputs of vision networks. This process takes “seed” images and uses this optimization process to modify them to produce high activations on the target neural network. This project gained significant attention because it tended to produce striking, psychedelic-like modifications to the original input images. For example: 

![deep dream image]( https://1.bp.blogspot.com/-CdUrPm7x5Ig/VZQIGjJzP0I/AAAAAAAAAnI/qhqchfzdaOc/s640/image00.jpg)

The next step in the development of circuits research was [Feature Visualization](https://distill.pub/2017/feature-visualization/). In this paper, the authors apply the visualization process to neurons within the network, not just the output like Deep Dream did. This is an important step because it allows us to learn about the mechanistic behavior inside the neural network at any level of depth/complexity, not just the high-level behavior of the whole thing. The authors mention that a variety of regularization techniques are required before this method produces coherent-looking results, otherwise the output tends to be full of high-frequency noise and resemble an adversarial example more than an interpretable feature visualization. They also discuss alternative methods of characterizing neuron behavior, such as selecting high-activation images. They conclude that visualization by optimization allows for a more precise understanding of a feature than simply selecting high-activation images because it removes features that may be irrelevant but correlated - especially when multiple diverse outputs can be compared to identify the commonality. For a great example of how feature visualization lets us learn about specific neurons within a network, and a comparison of these two interpretability methods, see [OpenAI Microscope]( https://microscope.openai.com/models).
 
Another important part of the circuits toolbox is [Visualizing Weights](https://distill.pub/2020/circuits/visualizing-weights/). This paper provides a method to visualize the weights connecting different layers, which can help us understand the relationships between the different features identified in different neurons in adjacent layers. The main technique is decomposing features into their corresponding weight vectors, however it assumes that there is little nonlinear interaction between layers. Still, the "expanded weights" method, which replaces non-linear operations with the closest linear operation, is able to identify meaningful interactions between layers. Especially when combined with feature visualization, it lets us see how the different components of the neural network interact. For example, a window detector might have a positive effect on the activation of a car detector when it’s near the top of the frame, but a negative effect when it’s near the bottom.
 
One of the most notable results to come out of the circuits literature is [Curve Circuits](https://distill.pub/2020/circuits/curve-circuits/): In this paper, researchers use the circuits method to isolate, identify, and group the steps InceptionV1 uses in early vision to detect different types of curves. They also use weight visualization to assess how those curve detectors interact, and assemble into larger, more complex features deeper in the network. Most importantly, they go on verify their claims by creating an artificial network from the isolated circuits that identifies curves in a very similar way to the original model. Finally, the researchers discuss the role these isolated circuits play in identifying more complex visual artifacts downstream in InceptionV1. Curve Circuits is an extremely important step in the circuits literature and one of the strongest indications that the circuits method is useful and legitimate because it allowed researchers to accurately reverse-engineer and re-implement by hand some of the algorithms used in a pre-trained vision network.
 
The paper we mean to respond to in this project is [Quantifying Interpretability](https://arxiv.org/abs/1704.05796). In this paper, the authors quantify the “interpretability” of different internal representations within neural networks. They compare the interpretability of features generated by maximizing the activations of single neurons (basis vectors in activation space) against features generated by maximizing linear combinations of activations (random vectors in activation space). They found that random vectors appear just as interpretable as basis vectors. This result is quite interesting, because it threatens the theoretical foundations for the usefulness of circuit analysis. If the basis vectors are not more human-interpretable than random vectors, it could mean that neural networks do not necessarily assign human-interpretable algorithms to particular neurons, and that trying to understand their behavior from this angle might be much harder than originally anticipated.

### Methods
We used [Lucid](https://github.com/tensorflow/lucid), which is built on top of the widely used open source machine learning project. Most of the code we used was written ourselves, but the main visualization method was based on feature visualization methods linked in the reproducibility notebooks from [this paper](https://distill.pub/2017/feature-visualization/). Lucid is a feature visualization package that allows us to visualize inputs that maximize a given objective function that we can define ourselves. This objective function can be the activation of a specific neuron, a specific channel, or an arbitrary linear combination of activations.

We ended up studying InceptionV1 off-the-shelf, trained on ImageNet. We chose this model because it is a relatively simple and well-understood open-source detection/classification vision model. It has been used as something like a "model organism" in the circuits literature so far. The data set for our study consists of the visualizations of optimal inputs for a given neuron in the network, as well as visualizations of random activations as a control group.

Since we are performing feature visualization by optimization (as described in our summary of Quantifying Interpretability), our inputs were floating-point unit vectors of activations, and our outputs were three-channel images that produce the desired activation pattern. These activation vectors were either randomly chosen basis vectors (one-hot for randomly chosen single neurons),  or the sum of randomly generated weighted vectors of activations of all neurons in the chosen layer, which produces a pseudo-random image. Examples of each are below: 

Basis activation:

![Basis activation example](https://i.imgur.com/LMLA10n.jpg)

Random activation: 

![Random activation example](https://i.imgur.com/6qD6AMz.jpg)


To generate these basis images, we defined an objective function as the activation of a randomly chosen neuron, and visualized the input that maximized this function using Lucid's rendering library. The code used to do this is below: 

```
# Basis Activation Image Generator
model = models.InceptionV1()
model.load_graphdef()
param_f = lambda: param.image(256)
for i in range(10):
  # We got lucky, no repeated images on the first try :)
  n = random.randint(0,831)
  obj = objectives.channel("mixed5a", n)
  res = render.render_vis(model, obj, param_f)
 ```


```
# Random Activation Image Generator
model = models.InceptionV1()
model.load_graphdef()
param_f = lambda: param.image(256)
for i in range(10):
  obj = 0
  weights = []
  for n in range(0,831):
    w = random.uniform(-1,1)
    obj += w * objectives.channel("mixed5a", n)
    weights.append(w)
    # Note that we saved the weights used for reproducibility purposes
  res = render.render_vis(model, obj, param_f)
 ```

In order to evaluate the interpretability of basis and random circuits, we created a survey using the MTurk Amazon service and received 40 responses from random workers who chose to complete the survey. The survey consists of 10 pairs of images: one image that was created from optimizing a basis activation vector, and one that was created from optimizing a random activation vector. Respondents were then asked “Which image is easier to interpret as a feature of an object, animal, or person?” Their answer to this question was considered as a measure of which image is more interpretable. 

### Results
The results of this survey can be found [here](https://www.surveymonkey.com/results/SM-PZVTYHVC9/). Between all 10 pairs of images, 185 of the basis images were chosen as being more interpretable and 215 of the random images were chosen as being more interpretable. This is actually slightly in the opposite direction of the result we expected to find - the basis images were chosen *fewer* times than the random images! For the statistical significance portion of this analysis, we will use a binomial test. This type of test is well-suited to our data: it deals with binary outputs, it can take into account that our null hypothesis is p=0.5, and it is an exact test so it will perform well with a small sample size. The calculation of the test shown [here](https://colab.research.google.com/drive/1HRIRBiMKNgf-2GZzxGu8wPAAdsEcT9_n) gives a p-value of 0.147. This means we cannot reject the null hypothesis at the standard 0.05 significance level. In other words, we did not find any significant difference in the human-reported interpretability of basis vs random feature visualizations. This does not not necessarily imply that there is no difference in the interpretability of images generated from basis activation circuits and random activation circuits. It only means this study failed to demonstrate any such difference.

To determine how much this result actually challenges the claims from the Circuits literature that individual neurons will correspond to more human-readable concepts than random, we calculated a statistical power curve. This will show the probability of finding a non-null result for any given effect size. For larger effect sizes, a null result such as ours becomes less and less probable. 

![power curve](https://i.imgur.com/bbXSM9d.png)

### Discussion
After analyzing the responses we received from our survey on basis versus random activation, we could see that the results were not statistically significant. This poses a challenge to the claim from the Circuits literature that we aimed to assess. How much of a challenge depends on the effect size being claimed. While we do not have an exact figure for the expected effect size, we can make rough claims such as "an effect size of 0.1 is still entirely possible, but an effect size of 1 is very unlikely." 

Of course, this also may be due to the construction of our survey, which was limited to 10 questions and 40 responses per question by the survey service we used. We could have done more research on services that provide survey creation to avoid the paywall that we hit in our survey, and chosen a service that was either cheaper or entirely free. The small sample size is without a doubt the largest limitation of our study. Additionally, we formatted our survey in the simplest possible way, with a “choose which is more interpretable” question between a basis and random activation picture. Restructuring the format could provide more insight into which is actually more interpretable. One idea is to have the people taking the survey describe what they see in a picture, then compare their responses to what the neuron is actually based on. For example, if the neuron detects dog faces, and people say they see a dog in the image, we will know that they are actually able to interpret the neuron. Forcing surveyees to choose between two images might create noise if both images in a pair are equally interpretable or uninterpretable. There were a few pairs that had 60-40 or 50-50 choices, which lets us know that they are equal in interpretability, but with only 10 questions, it becomes much more difficult to account for this equality in our statistics.

### Future Work
In the future, we could extend this research in a number of ways. If we continued with Mechanical Turk as a data source, we could improve our survey by asking more detailed questions, and adding validation questions to help us assess the quality of our data. This would also allow us to do more sophisticated statistical analysis, such as correlating data quality with likelihood of correctly classifying basis activations, or attempting to fit a power law to the frequency distribution of interpreted concepts. We could also attempt to train a convolutional neural network to distinguish between basis and random features - this loses the “human” element, but can provide a fairly clear result if there is in fact a signal to be found that can separate the two classes. Finally, we could attempt to extend this validation method to other layers, or other vision models.

### Demo Application
In addition to the research above, we developed an application that demonstrates how our survey worked. This application was developed using streamlit and the code for it can be found [here](https://github.com/theobayard/circuit_survey_app). The application creates and then displays two images: on images from a basis activation vector and one image from a random activation vector. These images have no indication of which of the two types they are, and their order is randomized, preventing even those familiar with the application from knowing which image is which. Furthermore, unique images will be generated every time the page is loaded. The user can then select which of the two images they believe is more interpretable. The user’s answer will then be saved and two new images will be generated to be interpreted. At the bottom of the page, there is also a graph that indicates how many of each type of image has been selected as being more interpretablable. 
![CircuitAppScreenShot](https://user-images.githubusercontent.com/41492964/145737904-44ea5398-7734-48c7-9e42-23b6406ad0ac.jpg)
