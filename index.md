## Validating the Human-Interpretability of Circuit Feature Visualization

### Project Members
- James Lucassen
- Josh Cordova
- Sofiane Dissem
- Theo Bayard de Volo

### Outline
Neural networks are generally considered black boxes, which introduces a number of problems. If we do not understand the "reasoning" inside a neural network, it makes it much harder to verify whether it is aligned, robust, or fair. [Previous research](https://distill.pub/2020/circuits/) has suggested that circuit analysis can make the black box of a neural net more transparent. The usefulness of circuit analysis relies on the idea that features which activate single neurons are more closely related to human-interpretable concepts than features which activate random combinations of neurons. Research has been done to verify this claim, but produced conflicting results. We propose to directly test this claim, by performing a survey to compare the perceived human-interpretability of single-neuron vs random features.

Significant results in ML transparency could have substantial ethical implications, for both modern data ethics and the long-term safety of AI. A more thorough body of research validating the promising but new Circuits method could bring a powerful new transparency tool to a more mature stage of development, ready for application. Regarding modern data ethics, transparency procedures are valuable for ensuring machine learning models act fairly before deployment, providing evidence in appeals against unfair machine decisions, and a variety of other applications. And in the long term, as AI systems are given more important roles and grow more complicated, transparency is likely to be crucial for designing validation processes which will ensure that they never fail catastrophically.

### Related Works
1. [Feature Visualization](https://distill.pub/2017/feature-visualization/): This paper describes one of the key methods of circuit analysis - feature visualization by image optimization. Visualization by optimization allows for a more precise understanding of a feature than simply selecting high-activation images because it removes features that may be irrelevant but correlated - especially when multiple diverse outputs can be compared to identify the commonality. The authors highlight that a variety of regularization techniques are required before this method produces coherent results, otherwise the output tends to be full of high-frequency noise and resemble an adversarial example more than a feature visualization.
2. [Curve Circuits](https://distill.pub/2020/circuits/curve-circuits/): In this paper researchers use neural circuits to isolate, identify, and group the steps InceptionV1 uses to detect curves. They then verify their claims by creating an artificial network from the isolated circuits that identifies curves in a very similar way to the original model. Finally, the researchers discuss the role these isolated circuits play in identifying more complex visual artifacts downstream in InceptionV1.
3. [Branch Specialization](https://distill.pub/2020/circuits/branch-specialization/): This paper details branch specialization, which is one of the three larger structural phenomena observed in neural networks. This phenomena occurs when neural net layers split into branches. The authors describe the implicit branching that forms in residual networks, as layers become two-way branches that can combine through addition. These branches combine to specialize in certain detection, such as the mixed3a_5x5, which specializes in color detection. Finally, the authors go into the consistency with which branch specialization occurs, as it occurs in almost every neural net, no matter how many times it is retrained.
4. [Visualizing Weights](https://distill.pub/2020/circuits/visualizing-weights/): This paper describes multiple techniques for specifically visualizing weights.
The main technique is decomposing features into their corresponding weight vectors, however it assumes that there is little nonlinear interaction between layers. Still, the "expanded weights" method, which replaces non-linear operations with the closest linear operation, is able to identify meaningful interactions between layers.
5. [Quantifying Interpretability](https://arxiv.org/abs/1704.05796): In this paper, the authors quantify the “interpretability” of different internal representations within neural networks.They compare the interpretability of features generated by maximizing the activations of single neurons against features generated by maximizing linear combinations of activations - these correspond roughly to basis vectors and random vectors in the layer’s activation space. They found that random vectors appear just as interpretable as basis vectors. This result is quite interesting, because it threatens the theoretical foundations for the usefulness of circuit analysis.

### Methods
We used Lucid, which is built on top of the widely used open source machine learning project. Most of the code was written ourselves, but the main visualization method was based on feature visualization methods linked in the reproducibility notebooks from [this paper](https://distill.pub/2017/feature-visualization/). Lucid is a feature visualization package that allows us to visualize inputs that maximize a given objective function that we can define ourselves. This objective function can be the activation of a specific neuron, a specific channel, or an arbitrary linear combination of activations.

We ended up studying InceptionV1 off-the-shelf, trained on ImageNet. We chose this model because it is a relatively simple and well-understood open-source detection/classification vision model. It has been used as something like a "model organism" in the circuits literature so far. To wit, our data set consists of the visualizations of optimal inputs for a given neuron in the network, as well as visualizations of random activations as a control group.

Since we are performing feature visualization by optimization, our inputs were floating-point unit vectors of activations, and our outputs were three-channel images that produce the desired activation pattern. These activation vectors were either randomly chosen basis vectors (one-hot for randomly chosen single neurons) or the sum of randomly generated weighted vectors of activations of all neurons in the chosen layer.

In order to evaluate the interpretability of basis and random circuits, we created a survey using the MTurk Amazon service and received 40 responses from random workers who chose to complete the survey. The survey consists of 10 pairs of images: one image that was created from a basis circuit, and one that was created from a random activation circuit. Respondents were then asked “Which image is easier to interpret as a feature of an object, animal, or person?” Their answer to this question was considered as a measure of which image is more interpretable. 

After acquiring the survey results, we determined if there was a statistically significant difference between the number of basis images and random images that were chosen. This was done via a binomial test. Any p value less than .05 would be considered significant. 

### Results
40 subjects participated in our MTurk survey. The results of this survey can be found [here](https://www.surveymonkey.com/results/SM-PZVTYHVC9/). Between all 10 pairs of images, 185 of the basis images were chosen as being more interpretable and 215 of the random images were chosen as being more interpretable. The binomial test of our data, shown [here](https://colab.research.google.com/drive/1nfFCNOSHp4mHXqQ-MpeQyWV5OV7pRGHx?usp=sharing), gives a p-value of 0.147. So, we did not find any significant difference in the interetability of basis and random activation circuits. This does not imply that there is no difference in the interpretability of images generated from basis activation circuits and random activation circuits. It only means this study failed to demonstrate any such difference.

### Results/Discussion
After analyzing the responses we received from our survey on basis versus random activation, we could see that the results were not statistically significant. This may be due to the construction of our survey, which was limited to 10 questions and 40 responses per question by the survey service we used. We could have done more research on services that provide survey creation to avoid the paywall that we hit in our survey, and chosen a service that was either cheaper or entirely free. Additionally, we formatted our survey in the simplest possible way, with a “choose which is more interpretable” question between a basis and random activation picture. Restructuring the format could provide more insight into which is actually more interpretable. One idea is to have the people taking the survey describe what they see in a picture, then compare their responses to what the neuron is actually based on. For example, if the neuron detects dog faces, and people say they see a dog in the image, we will know that they are actually able to interpret the neuron. Forcing surveyees to choose between two images might create noise if both images in a pair are equally interpretable or uninterpretable. There were a few pairs that had 60-40 or 50-50 choices, which lets us know that they are equal in interpretability, but with only 10 questions, it becomes much more difficult to account for this equality in our statistics.

### Future Work

In the future, we could extend this research in a number of ways. If we continued with Mechanical Turk as a data source, we could improve our survey by asking more detailed questions, and adding validation questions to help us assess the quality of our data. This would also allow us to do more sophisticated statistical analysis, such as correlating data quality with likelihood of correctly classifying basis activations, or attempting to fit a power law to the frequency distribution of interpreted concepts. We could also attempt to train a convolutional neural network to distinguish between basis and random features - this loses the “human” element, but can provide a fairly clear result if there is in fact a signal to be found that can separate the two classes. Finally, we could attempt to extend this validation method to other layers, or other vision models.
