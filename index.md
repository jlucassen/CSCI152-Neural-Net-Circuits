## Inspecting Neural Network Reasoning Using Circuits

### Project Members
- James Lucassen
- Josh Cordova
- Sofiane Dissem
- Theo Bayard de Volo

### Outline
Neural networks are generally considered black boxes, which introduces a number of problems. If we do not understand the "reasoning" inside a neural network, it makes it much harder to verify whether it is aligned, robust, or fair. [Previous research](https://distill.pub/2020/circuits/) has suggested that circuit analysis can make the black box of a neural net more transparent. The usefulness of circuit analysis relies on the idea that features which activate single neurons are more closely related to interpretable concepts than features which activate random combinations of neurons. Research has been done to verify this claim, but produced conflicting results. We propose a new method for testing this claim by directly measuring interpretability as humans perceive it. Using this method, we will evaluate the validity of the main assumption underpinning circuit analysis.

If our results are significant, they could have substantial implications for data ethics. Validating a circuits-based approach towards transparency in neural networks would justify further research into this area of study. Transparency procedures are valuable for ensuring machine learning models act fairly before deployment, providing evidence in appeals against unfair machine decisions, and a variety of other applications.

### Related Works
1. [Feature Visualization](https://distill.pub/2017/feature-visualization/): This paper describes one of the key methods of circuit analysis - feature visualization by image optimization. Visualization by optimization allows for a more precise understanding of a feature than simply selecting high-activation images because it removes features that may be irrelevant but correlated - especially when multiple diverse outputs can be compared to identify the commonality. The authors highlight that a variety of regularization techniques are required before this method produces coherent results, otherwise the output tends to be full of high-frequency noise and resemble an adversarial example more than a feature visualization.
2. [Curve Circuits](https://distill.pub/2020/circuits/curve-circuits/): In this paper researchers use neural circuits to isolate, identify, and group the steps InceptionV1 uses to detect curves. They then verify their claims by creating an artificial network from the isolated circuits that identifies curves in a very similar way to the original model. Finally, the researchers discuss the role these isolated circuits play in identifying more complex visual artifacts downstream in InceptionV1.
3. [Branch Specialization](https://distill.pub/2020/circuits/branch-specialization/): This paper details branch specialization, which is one of the three larger structural phenomena observed in neural networks. This phenomena occurs when neural net layers split into branches. The authors describe the implicit branching that forms in residual networks, as layers become two-way branches that can combine through addition. These branches combine to specialize in certain detection, such as the mixed3a_5x5, which specializes in color detection. Finally, the authors go into the consistency with which branch specialization occurs, as it occurs in almost every neural net, no matter how many times it is retrained.
4. [Visualizing Weights](https://distill.pub/2020/circuits/visualizing-weights/): This paper describes multiple techniques for specifically visualizing weights.
The main technique is decomposing features into their corresponding weight vectors, however it assumes that there is little nonlinear interaction between layers. Still, the "expanded weights" method, which replaces non-linear operations with the closest linear operation, is able to identify meaningful interactions between layers.
5. [Quantifying Interpretability](https://arxiv.org/abs/1704.05796): In this paper, the authors quantify the “interpretability” of different internal representations within neural networks.They compare the interpretability of features generated by maximizing the activations of single neurons against features generated by maximizing linear combinations of activations - these correspond roughly to basis vectors and random vectors in the layer’s activation space. They found that random vectors appear just as interpretable as basis vectors. This result is quite interesting, because it threatens the theoretical foundations for the usefulness of circuit analysis. 

### Methods
We plan to use PyTorch software. Most of our tools will be written ourselves, but likely heavily based on feature visualization methods linked in the reproducibility notebooks from [this paper](https://distill.pub/2017/feature-visualization/).

We will be studying InceptionV1 off-the-shelf, trained on ImageNet. We chose this model because it is a relatively simple and well-understood open-source detection/classification vision model. It has been used as something like a "model organism" in the circuits literature so far. To wit, our data set will be the visualizations of optimal inputs for a given neuron in the network, as well as visualizations of random activations as a control group.

Since we are performing feature visualization by optimization, our inputs will be floating-point unit vectors of activations, and our outputs will be three-channel images that produce the desired activation pattern.
